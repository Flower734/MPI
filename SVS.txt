#include <mpi.h>
#include <fstream>
#include <vector>
#include <math.h>
#include <algorithm>
#include <stdio.h>
#include <time.h>
#include <sstream>
#include <iostream>
#include <string.h>
#include <omp.h>
#define MAXSIZE 2000
#define NUM_OF_THREAD 4
using namespace std;
class index {
public:
	int len = 0;
	vector<unsigned int> arr;
};

void sorted(int* list, vector<index>& idx, int num) {
	for (int i = 0; i < num - 1; i++) {
		for (int j = 0; j < num - i - 1; j++) {
			if (idx[list[j]].len > idx[list[j + 1]].len) {
				int tmp = list[j];
				list[j] = list[j + 1];
				list[j + 1] = tmp;
			}
		}
	}
}
unsigned int* interaction(unsigned int* a, vector<unsigned int> b) {//两个队列求交
	int len = sizeof(a);
	int b_len = b.size();
	for (int i = 0; i < len; i++) {
		bool isExit = false;
		for (int j = 0; j < b_len; j++) {
			if (a[i] == b[j]) {
				isExit = true;
				break;
			}
		}
		if (isExit == false) {
			a[i] = -1;
		}
	}

	return a;
}
vector<unsigned int>& SVS(int* list, vector<index>& idx, int num) {
	MPI_Init(NULL, NULL);
	int rank, size;
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	MPI_Comm_size(MPI_COMM_WORLD, &size);
	int len = idx[list[0]].arr.size();
	int length = floor(len / size);
	vector<unsigned int> arr;
	unsigned int* s;
	if (rank == 0) {
		s = new unsigned int[len - length * (size - 1)];
		for (int i = 0; i < len - length * (size - 1); i++) {
			s[i] = idx[list[0]].arr[i + length * (size - 1)];
		}
	}
	else {
		s = new unsigned int[length];
		for (int i = 0; i < length; i++) {
			s[i] = idx[list[0]].arr[i + length * (rank - 1)];
		}
	}
	for (int i = 1; i < num; i++) {//num个列表求交
		s = interaction(s, idx[list[i]].arr);
	}
	if (rank == 0) {
		arr.assign(s, s + sizeof(s));
		for (int i = 1; i < size; i++) {
			MPI_Recv(&s, length, MPI_UNSIGNED, i, 1, MPI_COMM_WORLD, MPI_STATUSES_IGNORE);
			arr.insert(arr.end(), s, s + length);
		}
	}
	else {
		MPI_Send(&s, length, MPI_UNSIGNED, 0, 1, MPI_COMM_WORLD);
	}
	MPI_Finalize();
	return arr;
}
vector<unsigned int>& SVS_unlock_P(int* list, vector<index>& idx, int num) {
	vector<unsigned int> s;
	int provided;
	MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &provided);
	if (provided < MPI_THREAD_FUNNELED)
		MPI_Abort(MPI_COMM_WORLD, 1);
	int rank, p;
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	MPI_Comm_size(MPI_COMM_WORLD, &p);
	s = idx[list[0]].arr;
	int length = s.size();
	length = floor(length / 3);
	vector<unsigned int> a, b;
	if (rank < num) {
		int s_len = s.size();
#pragma omp parallel for num_threads(size)\
        shared(t,s_len,idx,s,i) ,schedule(guided,500)
		// schedule(static,s_len/THREADS_NUM)
		for (int i = 0; i < s_len; i++) {//将这部分循环划分给子线程
			bool isExit = false;
#pragma omp simd
			for (int j = 0; j < idx[list[rank]].arr.size(); j++) {
				if (s[i] == idx[list[rank]].arr[j]) {
					isExit = true;
					break;
				}
			}
			if (isExit == false) {
				//#pragma omp atomic write
				//#pragma omp critical
				s[i] = -1;//不存在就把s[i]设置为-1
			}
		}
		vector<unsigned int>::iterator newEnd(remove(s.begin(), s.end(), -1));
		s.erase(newEnd);
	}
	if (rank == 0) {//接受各进程中的数据
		vector<unsigned int> tmp;
		int size;
		MPI_Status status;
		MPI_Request** request = new MPI_Request * [p - 1];
		for (int i = 0; i < p - 1; i++) { request[i] = new MPI_Request[2]; }
		for (int i = 1; i < size; i++) {
			MPI_Irecv(&size, 1, MPI_INT, MPI_ANY_SOURCE, 1, MPI_COMM_WORLD, &request[i - 1][0]);
			MPI_Irecv(&tmp, size, MPI_UNSIGNED, status.MPI_SOURCE, 2, MPI_COMM_WORLD, &request[i - 1][1]);
			MPI_Waitall(2, request[i - 1], &status);
			s = interaction(s, tmp);//求交
		}
	}
	else {
		int size = s.size();
		MPI_Request request[2];
		MPI_Isend(&size, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, &request[0]);
		MPI_Isend(&s, size, MPI_UNSIGNED, 0, 2, MPI_COMM_WORLD, &request[1]);
		MPI_Waitall(2, request, MPI_STATUS_IGNORE);
	}
	MPI_Finalize();
	return s;
}
index& Hash(int* list, vector<index>& idx, int num) {
	vector<unsigned int> s;
	int provided;
	MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &provided);
	if (provided < MPI_THREAD_FUNNELED)
		MPI_Abort(MPI_COMM_WORLD, 1);
	int rank, p;
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	MPI_Comm_size(MPI_COMM_WORLD, &p);
	s = idx[list[0]].arr;
	int length = s.size();
	length = floor(length / 3);
	vector<unsigned int> a, b;
	if (rank < num) {
		int count = 0;
#pragma omp parallel for num_threads(4) //schedule(guided,400)
		{
			for (int j = 0; j < length; j++) {//s列表中的各个值比较
				bool isExit = false;
				int a = floor(s[count] / 65536);
				int t = HL[list[i]][a].beg;
				int t1 = HL[list[i]][a].end;
				if (t < 0 || t1 < 0) {
#pragma omp atomic
					s.erase(s.begin() + count);
					continue;
				}
				else {
					for (t; t <= t1; t++) {//第i个列表的各个值与s列表中的第【j]个值比较
						if (s[count] == idx[list[i]].arr[t]) {
							isExit = true;
							break;
						}
					}
					if (isExit == false) {
#pragma omp atomic
						s.erase(s.begin() + count);//删除第s中第j个元素
					}
					else {
						count++;
					}
				}
			}
		}
	}
	if (rank == 0) {//接受各进程中的数据
		vector<unsigned int> tmp;
		int size;
		MPI_Status status;
		MPI_Request** request = new MPI_Request * [p - 1];
		for (int i = 0; i < p - 1; i++) { request[i] = new MPI_Request[2]; }
		for (int i = 1; i < size; i++) {
			MPI_Irecv(&size, 1, MPI_INT, MPI_ANY_SOURCE, 1, MPI_COMM_WORLD, &request[i - 1][0]);
			MPI_Irecv(&tmp, size, MPI_UNSIGNED, status.MPI_SOURCE, 2, MPI_COMM_WORLD, &request[i - 1][1]);
			MPI_Waitall(2, request[i - 1], &status);
			s = interaction(s, tmp);//求交
		}
	}
	else {
		int size = s.size();
		MPI_Request request[2];
		MPI_Isend(&size, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, &request[0]);
		MPI_Isend(&s, size, MPI_UNSIGNED, 0, 2, MPI_COMM_WORLD, &request[1]);
		MPI_Waitall(2, request, MPI_STATUS_IGNORE);
	}
	MPI_Finalize();
	return s;
}
index& SVS_omp(int* list, vector<index>& idx, int num) {//7195   simd:6501
	vector<unsigned int> s = idx[list[0]].arr;
	int i = 0;
	for (int t = 1; t < num; t++) {//s依次与quer求交
		int s_len = s.arr.size();
#pragma omp parallel for num_threads(4)\
        shared(t,s_len,idx,s,i) ,schedule(guided,500)
		// schedule(static,s_len/THREADS_NUM)
		for (i; i < s_len; i++) {//将这部分循环划分给子线程
			bool isExit = false;
#pragma omp simd
			for (int j = 0; j < idx[list[t]].arr.size(); j++) {
				if (s.arr[i] == idx[list[t]].arr[j]) {
					isExit = true;
					break;
				}
			}
			if (isExit == false) {
				//#pragma omp atomic write
				//#pragma omp critical
				s.arr[i] = -1;//不存在就把s[i]设置为-1
			}
		}
		vector<unsigned int>::iterator newEnd(remove(s.arr.begin(), s.arr.end(), -1));
		s.arr.erase(newEnd);
	}

	return s;
}
vector<unsigned int>& SVS_P(int* list, vector<index>& idx, int num) {
	vector<unsigned int> s = idx[list[0]].arr;
	int provided;
	MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &provided);
	if (provided < MPI_THREAD_FUNNELED)
		MPI_Abort(MPI_COMM_WORLD, 1);
	int rank;
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	s = idx[list[0]].arr;
	int length = s.size();
	length = floor(length / 3);
	vector<unsigned int> a, b;
	if (rank < num) {
		int s_len = s.size();
#pragma omp parallel for num_threads(size)\
        shared(t,s_len,idx,s,i) ,schedule(guided,500)
		// schedule(static,s_len/THREADS_NUM)
		for (int i = 0; i < s_len; i++) {//将这部分循环划分给子线程
			bool isExit = false;
#pragma omp simd
			for (int j = 0; j < idx[list[rank]].arr.size(); j++) {
				if (s[i] == idx[list[rank]].arr[j]) {
					isExit = true;
					break;
				}
			}
			if (isExit == false) {
				//#pragma omp atomic write
				//#pragma omp critical
				s[i] = -1;//不存在就把s[i]设置为-1
			}
		}
		vector<unsigned int>::iterator newEnd(remove(s.begin(), s.end(), -1));
		s.erase(newEnd);
	}
	if (rank == 0) {//接受各进程中的数据
		vector<unsigned int> tmp;
		int size;
		MPI_Status status;
		for (int i = 1; i < size; i++) {
			MPI_Recv(&size, 1, MPI_INT, MPI_ANY_SOURCE, 1, MPI_COMM_WORLD, &status);
			MPI_Recv(&tmp, size, MPI_UNSIGNED, status.MPI_SOURCE, 2, MPI_COMM_WORLD, &status);
			s = interaction(s, tmp);//求交
		}
	}
	else {
		int size = s.size();
		MPI_Send(&size, 1, MPI_INT, 0, 1, MPI_COMM_WORLD);
		MPI_Send(&s, s.size(), MPI_UNSIGNED, 0, 2, MPI_COMM_WORLD);
	}
	MPI_Finalize();
	return s;
}


int main(int argc, char* argv[]) {
	//读取二进制文件
	fstream file;
	file.open("ExpIndex", ios::binary | ios::in);
	if (!file.is_open()) {
		cout << "No file";
		return 0;

	}
	vector<index> idx;
	for (int i = 0; i < 2000; i++) {
		index temp;
		file.read((char*)&temp.len, sizeof(temp.len));
		for (int j = 0; j < (temp.len); j++)
		{
			unsigned int binary_temp;
			file.read((char*)&binary_temp, sizeof(binary_temp));
			temp.arr.push_back(binary_temp);
		}
		idx.push_back(temp);
	}
	file.close();
	file.open("ExpQuery", ios::in);
	int query[1000][5] = { 0 };
	string line;
	int count = 0;
	while (getline(file, line)) {
		stringstream ss; //输入流
		ss << line; //向流中传值
		int a = 0;
		while (!ss.eof()) {
			int temp;
			ss >> temp;
			query[count][a] = temp;
			a++;
		}
		count++;
	}


	//struct timespec sts, ets;
	//timespec_get(&sts, TIME_UTC);
	clock_t start, _end;
	start = clock();

	int num = 0;
	for (int j = 0; j < 5; j++) {
		if (query[1][j] != 0) {
			num++;
		}
	}
	int* list = new int[num];//例子中的数
	for (int j = 0; j < num; j++) {
		list[j] = query[1][j];
	}
	//------ - 按表求交-------- -
	sorted(list, idx, num);
	SVS(list, idx, num);
	delete list;

	_end = clock();
	cout << (_end - start);
	/*timespec_get(&ets, TIME_UTC);
	time_t dsec = ets.tv_sec - sts.tv_nsec;
	long dnsec = ets.tv_nsec - sts.tv_nsec;
	printf(" % llu. % 09llus\n", dsec, dnsec);*/
	return 0;
}